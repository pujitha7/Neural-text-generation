{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#loading libraries\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "batch_size = 32\n",
    "word_embedding_size = 80\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "JtMGasiEKttA"
   },
   "outputs": [],
   "source": [
    "#Basic preprocessing on the text\n",
    "\n",
    "f = open('war_peace.txt', 'r',encoding='utf8')\n",
    "x = f.read()\n",
    "f.close()\n",
    "x=x.split(\"\\n\\n\")\n",
    "for i in range(0,len(x)):\n",
    "    x[i]=x[i].replace(\"\\n\",\" \")\n",
    "new_text=[]\n",
    "for i in range(0,len(x)):\n",
    "    text=x[i].split(\". \")\n",
    "    for j in range(0,len(text)):\n",
    "        new_text.append(text[j])\n",
    "for i in range(0,len(new_text)):\n",
    "    new_text[i]=new_text[i].lower()\n",
    "new_txt = []\n",
    "for i in range(0,len(new_text)):\n",
    "    if(78>len(new_text[i].split(\" \"))):\n",
    "        new_txt.append(new_text[i])  \n",
    "train_data = []\n",
    "for i in range(0,len(new_txt)):\n",
    "    if len(new_text[i].split())>1:\n",
    "        train_data.append('<start> '+ new_txt[i] + \" <end>\")\n",
    "for i in range(0,len(train_data)):\n",
    "    train_data[i]=train_data[i].split(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#doing a train and test split\n",
    "\n",
    "test_data = train_data[26773:]\n",
    "train_data = train_data[:26773]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "P6Ln5_q5KttN"
   },
   "outputs": [],
   "source": [
    "########## To train your own Word2Vec model #################\n",
    "#model_w2v = Word2Vec(train_data,size = word_embedding_size,window = 5,min_count = 1,iter = 10)\n",
    "#model_w2v_tes = Word2Vec(train_data,size = word_embedding_size,window=5, min_count=2,iter = 10)\n",
    "\n",
    "######## To save your Word2Vec model ########\n",
    "#model_w2v.save('word2vec.mode')\n",
    "#model_tes.save('word2vec_tes.model')\n",
    "\n",
    "####### Loading the word2vec model already in memory #######\n",
    "model_w2v = Word2Vec.load(\"word2vec.model\")\n",
    "model_w2v_tes = Word2Vec.load(\"word2vec_tes.model\")\n",
    "\n",
    "vocab = list(model_w2v.wv.vocab)\n",
    "a = list(range(1,len(vocab)+1))\n",
    "vocab_test = list(model_w2v_tes.wv.vocab)\n",
    "vocab_dict = dict(zip(a,vocab))\n",
    "vocab_dict_inv = dict(zip(vocab,a))\n",
    "dif = list(set(list(model_w2v.wv.vocab))-set(vocab_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## function to convert the data into fixed length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv(data):\n",
    "    train = np.zeros([len(data),80,1],dtype=np.int64)\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data[i])):\n",
    "            try:\n",
    "                train[i][j][0] = vocab_dict_inv[data[i][j]]\n",
    "            except KeyError:\n",
    "                train[i][j][0] = vocab_dict_inv[random.choice(dif)]\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## invoking the conv function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "final_train = conv(train_data)\n",
    "final_test = conv(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## instead of padding all the words to max length in the dataset just padding the sentenses to max length in batch.\n",
    "## this function also converts words to word2vec embeddings.\n",
    "\n",
    "def proces_on_batch(data):\n",
    "    data_update = []\n",
    "    data = np.array(data)\n",
    "    for i in range(len(data)):\n",
    "        data_update.append(list(np.array(data[i]).reshape([80]))[:list(np.array(data[i]).reshape([80])).index(4) + 1])\n",
    "    \n",
    "    max_len_in_batch = len(max(data_update, key=len))\n",
    "    \n",
    "    train = np.zeros([batch_size,max_len_in_batch,word_embedding_size])\n",
    "    target = np.zeros([batch_size,max_len_in_batch,len(vocab)+1])\n",
    "    for k in range(0,batch_size):\n",
    "        for m in range(max_len_in_batch):\n",
    "            target[k][m][0] = 1\n",
    "    zeros = np.zeros([word_embedding_size])\n",
    "    \n",
    "    for i in range(len(data_update)):\n",
    "        for j in range(len(data_update[i])-1):\n",
    "            target[i][j][0] = 0\n",
    "            target[i][j][data_update[i][j+1]] = 1\n",
    "            train[i][j] = model_w2v.wv[vocab_dict[data_update[i][j]]]\n",
    "            \n",
    "    return train,target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## creating the data input pipe line and creating the iterators\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(final_train)\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(final_test)\n",
    "dataset = dataset.batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "test_iterator = test_dataset.make_one_shot_iterator()\n",
    "iterator = dataset.make_one_shot_iterator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## initializing the optimizer\n",
    "optimzer = tf.train.AdamOptimizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## LSTM states for use during inference\n",
    "lstm_1_ht = tf.contrib.eager.Variable(np.zeros([1,128]),dtype=tf.float32)\n",
    "lstm_1_ct = tf.contrib.eager.Variable(np.zeros([1,128]),dtype=tf.float32)\n",
    "lstm_2_ht = tf.contrib.eager.Variable(np.zeros([1,128]),dtype=tf.float32)\n",
    "lstm_2_ct = tf.contrib.eager.Variable(np.zeros([1,128]),dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## keras.Model class definition and all the variables in this class will be trained by the optimizer\n",
    "\n",
    "class language_model(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(tf.keras.Model,self).__init__()\n",
    "        self.LSTM_1 = tf.keras.layers.LSTM(128,return_sequences = True,\n",
    "                                        recurrent_initializer= tf.keras.initializers.truncated_normal(stddev=0.1),\n",
    "                                           recurrent_regularizer = tf.keras.regularizers.l2(0.01),\n",
    "                                           kernel_initializer=tf.keras.initializers.truncated_normal(stddev=0.1),\n",
    "                                          bias_initializer='zeros',kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                                           bias_regularizer = tf.keras.regularizers.l2(0.01),return_state = True)\n",
    "        \n",
    "        self.LSTM_2 = tf.keras.layers.LSTM(128,return_sequences = True,\n",
    "                                           recurrent_initializer = tf.keras.initializers.truncated_normal(stddev=0.1),\n",
    "                                           recurrent_regularizer = tf.keras.regularizers.l2(0.01),\n",
    "                                           kernel_initializer=tf.keras.initializers.truncated_normal(stddev=0.1),\n",
    "                                          bias_initializer='zeros',kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                                           bias_regularizer = tf.keras.regularizers.l2(0.01),return_state = True)\n",
    "        \n",
    "        \n",
    "        self.out = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(len(vocab)+1,\n",
    "                                                kernel_initializer=tf.keras.initializers.truncated_normal(stddev=0.1),\n",
    "                                                bias_initializer='zeros',\n",
    "                                                kernel_regularizer=tf.keras.regularizers.l2(0.01),\n",
    "                                               bias_regularizer = tf.keras.regularizers.l2(0.01)))\n",
    "        \n",
    "        self.lstm1_ht = tf.contrib.eager.Variable(np.zeros([batch_size,128]),dtype=tf.float32,name='LSTM_1_ht')\n",
    "        self.lstm1_ct = tf.contrib.eager.Variable(np.zeros([batch_size,128]),dtype=tf.float32,name='LSTM_1_ct')\n",
    "        self.lstm2_ht = tf.contrib.eager.Variable(np.zeros([batch_size,128]),dtype=tf.float32,name='LSTM_2_ht')\n",
    "        self.lstm2_ct = tf.contrib.eager.Variable(np.zeros([batch_size,128]),dtype=tf.float32,name='LSTM_2_ct')\n",
    "        \n",
    "    def main_model(self,train_values,state):\n",
    "        \n",
    "        global lstm_1_ht\n",
    "        global lstm_1_ct\n",
    "        global lstm_2_ht\n",
    "        global lstm_2_ct\n",
    "        \n",
    "        if state == \"train\":\n",
    "            x,_,_ = self.LSTM_1(train_values,initial_state = [self.lstm1_ht,self.lstm1_ct] )\n",
    "            x,_,_ = self.LSTM_2(x,initial_state = [self.lstm2_ht,self.lstm2_ct] )\n",
    "            x = self.out(x)\n",
    "            \n",
    "            return x\n",
    "        \n",
    "        else:\n",
    "            x,lstm_1_ht,lstm_1_ct = self.LSTM_1(train_values,initial_state = [lstm_1_ht,lstm_1_ct])\n",
    "            x,lstm_2_ht,lstm_2_ct = self.LSTM_2(x,initial_state = [lstm_2_ht,lstm_2_ct] )\n",
    "            x = self.out(x)\n",
    "            \n",
    "            return x\n",
    "        \n",
    "model = language_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## loss function definition need to use gradient tape which calculating the gradients and then passing the gradients to optimizer\n",
    "def loss_fun(train_batch,target):\n",
    "    with tf.GradientTape() as t:\n",
    "        loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(labels=target,logits=model.main_model(train_batch,\"train\")))\n",
    "    grads = t.gradient(loss,model.variables)\n",
    "    optimzer.apply_gradients(zip(grads,model.variables))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from weights61\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Variable '' shape=() dtype=int64, numpy=50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## restoring the weights of model if required and if already saved else you can train it\n",
    "\n",
    "model.main_model(tf.zeros([batch_size,60,word_embedding_size]),state=\"train\") ## just for the LSTM weights to be initialized so that values can be restored\n",
    "tf.contrib.eager.Saver.restore(file_prefix='weights61',self=tf.contrib.eager.Saver(var_list=list(model.variables)))\n",
    "global_step = tf.train.get_or_create_global_step()\n",
    "global_step.assign(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Creating a summary writer for writing files for tensorboard\n",
    "\n",
    "writer = tf.contrib.summary.create_file_writer('loss_lm')\n",
    "writer.set_as_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## function to capture loss (scalar summary) for tensorboard visualization\n",
    "\n",
    "def loss_viz(epoch_training_loss):\n",
    "    with tf.contrib.summary.always_record_summaries():\n",
    "        tf.contrib.summary.scalar(\"per_epoch_training_loss\",epoch_training_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## function for calculating perplexity\n",
    "\n",
    "def perplexity(test_temp=1,total=0,word_count = 0):\n",
    "    test_iterator = test_dataset.make_one_shot_iterator()\n",
    "    while test_temp == 1:\n",
    "        try:\n",
    "            test,test_target = proces_on_batch(test_iterator.get_next())\n",
    "            test = tf.cast(test,dtype=tf.float32)\n",
    "            test_target = tf.cast(test_target,dtype=tf.float32)\n",
    "            temp = model.main_model(test,state=\"train\")\n",
    "            temp = tf.nn.softmax(temp)\n",
    "            for i in range(len(np.array(temp))):\n",
    "                for j in range(len(np.array(temp[i]))):\n",
    "                    if np.argmax(test_target[i][j]) == 0:\n",
    "                        break\n",
    "                    else:\n",
    "                        total += np.log2(np.array(temp[i][j][np.argmax(test_target[i][j])]))\n",
    "                        word_count += 1\n",
    "        \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            total = -(total/word_count)\n",
    "            perplexity = pow(2,total)\n",
    "            print(\"perplexity\" , perplexity)\n",
    "            prep_list.append(total)\n",
    "            test_temp = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## training loop\n",
    "## once the iterator is done with one batch it will throw a OutOfRange exception will catch the exception and restart iterator\n",
    "## initializable iterator is not supported in tensorflow eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###Set training_required=True if you want to train the model\n",
    "training_required=False\n",
    "\n",
    "if training_required==True:\n",
    "\n",
    "    iterator = dataset.make_one_shot_iterator()\n",
    "    total_loss = 0\n",
    "    i = 50\n",
    "    while i < 62:\n",
    "        try:\n",
    "            train_batch = iterator.get_next()\n",
    "            train_batch,target = proces_on_batch(train_batch)\n",
    "\n",
    "            train_batch = tf.cast(train_batch,dtype=tf.float32)\n",
    "            target = tf.cast(target,dtype=tf.float32)\n",
    "\n",
    "            loss = loss_fun(train_batch,target)\n",
    "            total_loss += np.array(loss)\n",
    "        \n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print(\"loss for epoch \",i,\" is: \",total_loss)\n",
    "            iterator = dataset.make_one_shot_iterator()\n",
    "            global_step.assign_add(1)\n",
    "            loss_viz(total_loss)\n",
    "            tf.contrib.eager.Saver.save(file_prefix='weights' + str(i),self=tf.contrib.eager.Saver(var_list=list(model.variables)))\n",
    "            i = i + 1\n",
    "            total_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## inference loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#### initializing the cell state and hidden state ####\n",
    "lstm_1_ht = tf.reshape(model.lstm1_ht[0],shape=[1,128])\n",
    "lstm_1_ct = tf.reshape(model.lstm1_ct[0],shape=[1,128])\n",
    "lstm_2_ht = tf.reshape(model.lstm2_ht[0],shape=[1,128])\n",
    "lstm_2_ct = tf.reshape(model.lstm2_ct[0],shape=[1,128])\n",
    "h = 1\n",
    "\n",
    "#### initializing with <start> token\n",
    "current_word = (model_w2v.wv['<start>']).reshape([1,1,word_embedding_size])\n",
    "\n",
    "#### inference function\n",
    "def inference(current_word,search):\n",
    "    global h\n",
    "    current_word = model.main_model(current_word,\"inference\")\n",
    "    #### condition for greedy or random search\n",
    "    if search == 'greedy':\n",
    "        current_word = np.random.choice(np.argsort(np.array(((tf.nn.softmax(current_word[0][0])))))[-1:])\n",
    "    else:\n",
    "        current_word = np.random.choice(np.argsort(np.array(((tf.nn.softmax(current_word[0][0])))))[-5:])\n",
    "    if current_word == 0:\n",
    "        current_word = np.zeros([1,1,word_embedding_size])\n",
    "        print(\"<pad>\",end=\" \")\n",
    "    else:\n",
    "        current_word = vocab_dict[current_word]\n",
    "        if current_word == \"<end>\":\n",
    "            print(\"\\n\")\n",
    "            lstm_1_ht = tf.reshape(model.lstm1_ht[h],shape=[1,128])\n",
    "            lstm_1_ct = tf.reshape(model.lstm1_ct[h],shape=[1,128])\n",
    "            lstm_2_ht = tf.reshape(model.lstm2_ht[h],shape=[1,128])\n",
    "            lstm_2_ct = tf.reshape(model.lstm2_ct[h],shape=[1,128])\n",
    "            h += 1\n",
    "\n",
    "        else:\n",
    "            print(current_word,end=\" \")\n",
    "        current_word = (model_w2v.wv[current_word]).reshape([1,1,word_embedding_size])\n",
    "    \n",
    "    return current_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## random search inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "but i warn him this. in this world and can no change that he has not only a single idea \n",
      "\n",
      "re-establish the army and more just, \n",
      "\n",
      "i wish it is not possible \n",
      "\n",
      "i must prove that it will be very charmed for me or not harmful for you and not a fine thing!” replied nicholas \n",
      "\n",
      "her son and in the other \n",
      "\n",
      "the lady who had formerly judged his wife’s guilt, by his terms \n",
      "\n",
      "the army was accepted to all his wife’s career was to buy his respects and his wife’s commissioned had begun and "
     ]
    }
   ],
   "source": [
    "print(\"-----Random search predictions-----\")\n",
    "for i in range(0,100):\n",
    "    current_word = inference(tf.convert_to_tensor(current_word,dtype=tf.float32),search = 'random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## greedy search inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "to avoid his views but to the greatness that the welfare of the bee of the army was not to be ashamed of the city and the genius. or of the bee \n",
      "\n",
      "the wounded. \n",
      "\n",
      "the destruction of the stores were melting and the whole guard \n",
      "\n",
      "the rest of the 3rd corps of the french and the total show. \n",
      "\n",
      "the glory of the nation, and the shopkeepers, and the children of the commissariat of the semënov regiment, and of the russian army \n",
      "\n",
      "the glory of the liberal of the scene that he would not have liked "
     ]
    }
   ],
   "source": [
    "print(\"------Greedy Search predictions------\")\n",
    "for i in range(0,100):\n",
    "    current_word = inference(tf.convert_to_tensor(current_word,dtype=tf.float32),search = \"greedy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
